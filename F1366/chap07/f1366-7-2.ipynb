{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nfrom tensorflow.keras.datasets import cifar10\nfrom tensorflow.keras.utils import to_categorical\ndef prepare_data(): \n    \"\"\" \n    準備資料\n    Returns:\n        X_train(ndarray)     : 訓練資料 (50000,32,32,3)\n        X_test(ndarray)      : 測試資料 (10000,32,32,3) \n        y_train(ndarray)     : 將訓練資料的正確答案做One-hot encoding轉換 (50000,10) \n        y_test(ndarray)      : 將測試資料的正確答案做One-hot encoding轉換 (10000,10) \n        y_test_label(ndarray): 測試資料的正確答案 (10000)\n    \"\"\"\n    (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n\n    # 將用於訓練跟測試的圖像資料進行標準化\n    # 對4維張量所有軸線方向求出平均值、標準差 \n    # 可省略axis=(0,1,2,3)  \n    mean = np.mean(X_train, axis=(0,1,2,3)) \n    std = np.std(X_train, axis=(0,1,2,3)) \n    # 要執行標準化時，在分母的標準差加上極小值\n    x_train = (X_train - mean) / (std + 1e-7)\n    x_test = (X_test - mean) / (std + 1e-7)\n    # 將測試資料的正確答案拉直，從2維矩陣轉換為1維陣列\n    y_test_label = np.ravel(y_test) \n    # 將訓練資料與測試資料的正確答案標籤做One-hot encoding轉換 (變成10個類別)\n    y_train, y_test = to_categorical(y_train), to_categorical(y_test)\n\n    return X_train, X_test, y_train, y_test, y_test_label \n","metadata":{"execution":{"iopub.status.busy":"2021-05-23T06:15:04.602175Z","iopub.execute_input":"2021-05-23T06:15:04.603972Z","iopub.status.idle":"2021-05-23T06:15:10.001838Z","shell.execute_reply.started":"2021-05-23T06:15:04.603929Z","shell.execute_reply":"2021-05-23T06:15:10.000918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Input, Conv2D, Dense, Activation\nfrom tensorflow.keras.layers import AveragePooling2D, GlobalAvgPool2D\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.models import Model\n\ndef make_convlayer(input, fsize, layers): \n    \"\"\" \n    建立卷積層\n    Parameters: \n        inp(Input)  : 輸入層 \n        fsize(int)  : 過濾器尺寸 \n        layers(int) : 層數\n    Returns: 卷積層物件\n\"\"\"\n    x = input\n    for i in range(layers):\n        x = Conv2D(filters=fsize,\n                   kernel_size=3,\n                   padding=\"same\")(x)\n        x = BatchNormalization()(x)\n        x = Activation(\"relu\")(x)\n    return x\n\ndef create_model(): \n    \"\"\"\n    建立模型\n    Returns: 含有卷積層的模型\n    \"\"\"\n    input = Input(shape=(32,32,3))\n    x = make_convlayer(input, 64, 3)\n    x = AveragePooling2D(2)(x)\n    x = make_convlayer(x, 128, 3)\n    x = AveragePooling2D(2)(x)\n    x = make_convlayer(x, 256, 3)\n    x = GlobalAvgPool2D()(x)\n    x = Dense(10, activation=\"softmax\")(x)\n\n    model = Model(input, x)\n    return model\n","metadata":{"execution":{"iopub.status.busy":"2021-05-23T06:15:10.006043Z","iopub.execute_input":"2021-05-23T06:15:10.006368Z","iopub.status.idle":"2021-05-23T06:15:10.017536Z","shell.execute_reply.started":"2021-05-23T06:15:10.006341Z","shell.execute_reply":"2021-05-23T06:15:10.016559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy.stats import mode\n\ndef ensemble_majority(models, X):\n    \"\"\"\n    多數決集成\n    Parameters:\n        models(list): Model 物件清單\n        X(array): 驗證資料\n    Returns: np.ndarray收容了(10000)各個圖像的正確標籤\n    \"\"\"\n    # 製作(資料數量, 模型數量) 的零矩陣\n    pred_labels = np.zeros((X.shape[0],   # 列數同圖像張數\n                            len(models))) # 行數為模型數量\n    # 從models取出的索引值跟凍結更新的模型\n    for i, model in enumerate(models):\n        # 從每個模型的預測機率(資料數量 ,類別數量 ) 的各列(axis=1)當中\n        # 取最大值的索引(資料數量 ,模型數量 )\n        # 並在模型的行中放入各列的資料\n        pred_labels[:, i] = np.argmax(model.predict(X), axis=1)\n        # 在mode()中僅指定pred_labels的各列眾數為[0]並取得數據\n        # 將(資料數量, 1)的形狀以ravel()拉平為(, 資料數量)的形狀\n    return np.ravel(mode(pred_labels, axis=1)[0])\n","metadata":{"execution":{"iopub.status.busy":"2021-05-23T06:15:10.021076Z","iopub.execute_input":"2021-05-23T06:15:10.021351Z","iopub.status.idle":"2021-05-23T06:15:10.45051Z","shell.execute_reply.started":"2021-05-23T06:15:10.021324Z","shell.execute_reply":"2021-05-23T06:15:10.449611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import Callback\nclass Checkpoint(Callback): \n    \"\"\"Callback 的子類別\n    Attributes: \n        model(object): 訓練中的模型 \n        filepath(str): 儲存權重的資料夾路徑 \n        best_val_acc : 目前最高準確率\n    \"\"\"\n    def __init__(self, model, filepath): \n        \"\"\" \n        Parameters: \n            model(Model): 訓練中的模型\n            filepath(str): 儲存權重的資料夾路徑  \n            best_val_acc(int): 前最高準確率\n        \"\"\"\n        self.model = model\n        self.filepath = filepath\n        self.best_val_acc = 0.0\n\n    def on_epoch_end(self, epoch, logs): \n        \"\"\" \n        重新定義訓練週期結束時所呼出的函式\n        從剛剛的訓練週期當中儲存準確率較高的權重\n        Parameters:\n            epoch(int): 訓練次數\n            logs(dict): {'val_acc': 損失 , 'val_acc': 準確率 }\n        \"\"\"\n        if self.best_val_acc < logs['val_acc']:\n            # 儲存比前一次的訓練準確率還要高的權重\n            self.model.save_weights(self.filepath)\n            # 儲存準確率\n            self.best_val_acc = logs['val_acc']\n            print('Weights saved.', self.best_val_acc)\n","metadata":{"execution":{"iopub.status.busy":"2021-05-23T06:15:10.451889Z","iopub.execute_input":"2021-05-23T06:15:10.452244Z","iopub.status.idle":"2021-05-23T06:15:10.462055Z","shell.execute_reply.started":"2021-05-23T06:15:10.45219Z","shell.execute_reply":"2021-05-23T06:15:10.46105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\nimport pickle\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import LearningRateScheduler\nfrom tensorflow.keras.callbacks import History\ndef train(X_train, X_test, y_train, y_test, y_test_label):\n    \"\"\" \n    進行學習\n    Parameters:\n        X_train(ndarray): 訓練資料\n        X_test(ndarray) : 測試資料\n        y_train(ndarray): 訓練資料的正確答案\n        y_test(ndarray) : 測試資料的正確答案\n        y_test_label(ndarray): 測試資料的正確答案\n    \"\"\"\n    models_num = 5 # 集成的模型數量\n    batch_size = 1024 # 批次大小\n    epoch = 80 # 訓練次數\n    models = [] # 模型的清單\n    # 各個模型的訓練歷程dict \n    history_all = {\"hists\":[], \"ensemble_test\":[]} \n    # 初始化各個模型預測結果的2維矩陣\n    # (資料數量, 模型數量)\n    model_predict = np.zeros((X_test.shape[0], # 列數為圖像張數 \n                              models_num))     # 行數為模型數量 \n    # 模型有幾個、就重複幾次\n    for i in range(models_num):\n        # 顯示現在是第幾個模型 \n        print('Model',i+1) \n        # 建立卷積神經網路\n        train_model = create_model() \n        # 編譯模型\n        train_model.compile(optimizer='adam',\n                            loss='categorical_crossentropy',\n                            metrics=[\"acc\"])\n\n        # 將編譯後的模型追加到清單\n        models.append(train_model)\n\n        # 建立回呼當中的History物件\n        hist = History()\n        # 建立回呼當中的Checkpoint物件\n        cpont = Checkpoint(train_model,       # 模型\n                           f'weights_{i}.h5') # 儲存權重的檔名\n        # 步驟衰減函數t\n        def step_decay(epoch):\n            initial_lrate = 0.001 # 基礎學習率\n            drop = 0.5            # 衰減率\n            epochs_drop = 10.0    # 每10次訓練週期執行步驟衰減\n            lrate = initial_lrate * math.pow(drop,\n                                             math.floor((1+epoch)/epochs_drop))\n            return lrate\n\n        lrate = LearningRateScheduler(step_decay)\n\n        # 資料增補\n        datagen = ImageDataGenerator(rotation_range=15,      # 在15度範圍內隨機旋轉\n                                     width_shift_range=0.1,  # 以圖像寬度的0.1比例隨機橫向移動\n                                     height_shift_range=0.1, # 以圖像高度的0.1比例隨機上下移動\n                                     horizontal_flip=True,   # 朝水平方向隨機翻轉、左右對調\n                                     zoom_range=0.2)         # 以原始尺寸的0.2倍比例隨機放大\n        \n        # 進行訓練\n        train_model.fit(datagen.flow(X_train, y_train, batch_size=batch_size), \n                        epochs=epoch, \n                        steps_per_epoch=X_train.shape[0] // batch_size, \n                        validation_data=(X_test, y_test), \n                        verbose=1, \n                        callbacks=[hist, cpont, lrate]) # 回呼\n\n        # 讀入訓練完成的模型所獲得最高準確率時的權重\n        train_model.load_weights(f'weights_{i}.h5')\n        \n        # 凍結模型的所有權重更新\n        for layer in train_model.layers:\n            layer.trainable = False\n            \n        # 預測測試資料\n        # 求取每列的最大值\n        model_predict[:, i] = np.argmax(train_model.predict(X_test), axis=-1)\n        # 將訓練完成模型的訓練歷程登錄到history_all\n        history_all['hists'].append(hist.history)\n        # 執行多數決集成\n        ensemble_test_pred = ensemble_majority(models, X_test)\n        # 用scikit-learn.accuracy_score()取得集成的準確率\n        ensemble_test_acc = accuracy_score(y_test_label, ensemble_test_pred)\n        # 將集成準確率追加到global_hist\n        history_all['ensemble_test'].append(ensemble_test_acc) \n        # 輸出現在的集成精確度\n        print('Current Ensemble Accuracy : ', ensemble_test_acc)\n\n    history_all['corrcoef'] = np.corrcoef(model_predict, rowvar=False) # 求取每行的相關係數\n    print('Correlation predicted value')\n    print(history_all['corrcoef'])\n","metadata":{"execution":{"iopub.status.busy":"2021-05-23T06:15:10.463565Z","iopub.execute_input":"2021-05-23T06:15:10.464008Z","iopub.status.idle":"2021-05-23T06:15:10.638718Z","shell.execute_reply.started":"2021-05-23T06:15:10.463957Z","shell.execute_reply":"2021-05-23T06:15:10.637914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test, y_test_label = prepare_data()","metadata":{"execution":{"iopub.status.busy":"2021-05-23T06:15:10.64068Z","iopub.execute_input":"2021-05-23T06:15:10.64094Z","iopub.status.idle":"2021-05-23T06:15:16.441458Z","shell.execute_reply.started":"2021-05-23T06:15:10.640915Z","shell.execute_reply":"2021-05-23T06:15:16.440511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train(X_train, X_test, y_train, y_test, y_test_label)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T06:15:16.444279Z","iopub.execute_input":"2021-05-23T06:15:16.444547Z","iopub.status.idle":"2021-05-23T09:04:44.940146Z","shell.execute_reply.started":"2021-05-23T06:15:16.444521Z","shell.execute_reply":"2021-05-23T09:04:44.939062Z"},"trusted":true},"execution_count":null,"outputs":[]}]}