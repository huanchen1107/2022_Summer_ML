{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nfrom tensorflow.keras.datasets import cifar10\nfrom tensorflow.keras.utils import to_categorical\n\ndef prepare_data():\n    \"\"\"\n    準備資料\n    Returns:\n        X_train(ndarray): 訓練資料 (50000,32,32,3)\n        X_test(ndarray) : 測試資料 (10000,32,32,3)\n        y_train(ndarray): 將訓練資料作One-hot encoding轉換正確答案 (50000,10)\n        y_test(ndarray) : 將測試資料作One-hot encoding轉換正確答案 (50000,10)\n        y_test_label(ndarray): 測試資料的正確答案 (10000)\n    \"\"\"\n    (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n    # 對訓練用以及測試用的圖像資料執行標準化 (可省略axis=(0,1,2,3))\n    mean = np.mean(X_train,axis=(0,1,2,3))\n    std = np.std(X_train,axis=(0,1,2,3))\n    # 執行標準化時在分母的標準差加上極小值\n    x_train = (X_train-mean)/(std+1e-7)\n    x_test = (X_test-mean)/(std+1e-7)\n\n    # 將測試資料的正確答案先從2維矩陣拉直為1維陣列\n    y_test_label = np.ravel(y_test)\n    # 將訓練資料與測試資料的正確答案作One-hot encoding轉換 (轉換為10個類別)\n    y_train = to_categorical(y_train)\n    y_test = to_categorical(y_test)\n\n    return X_train, X_test, y_train, y_test, y_test_label\n","metadata":{"execution":{"iopub.status.busy":"2021-07-07T03:29:35.890171Z","iopub.execute_input":"2021-07-07T03:29:35.890669Z","iopub.status.idle":"2021-07-07T03:29:40.605595Z","shell.execute_reply.started":"2021-07-07T03:29:35.89057Z","shell.execute_reply":"2021-07-07T03:29:40.604761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Input, Conv2D, Dense, Activation\nfrom tensorflow.keras.layers import AveragePooling2D, GlobalAvgPool2D\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.models import Model\n\n\"\"\"\nbasic_conv_block1()\nbasic_conv_block2()\n建立卷積層\n\nParameters: \n    inp(Input): 輸入層\n    fsize(int): 過濾器大小\n    layers(int) : 層的數量\nReturns: Conv2D物件\n\"\"\"\ndef basic_conv_block1(inp, fsize, layers):\n    x = inp\n    for i in range(layers):\n        x = Conv2D(filters=fsize,\n                   kernel_size=3,\n                   padding=\"same\")(x)\n        x = BatchNormalization()(x)\n        x = Activation(\"relu\")(x)\n    return x\n\ndef basic_conv_block2(inp, fsize, layers):\n    weight_decay = 1e-4 # 超參數數值\n    x = inp\n    for i in range(layers):\n        x = Conv2D(filters=fsize,\n                   kernel_size=3,\n                   padding='same',\n                   kernel_regularizer=regularizers.l2(weight_decay) # 常規化\n                  )(x)\n        x = BatchNormalization()(x)\n        x = Activation('relu')(x)\n        return x\n\ndef create_cnn(model_num):\n    \"\"\"\n    建立模型\n    Parameters: \n        model_num(int): 模型編號\n    Returns: 模型\n    \"\"\"\n    inp = Input(shape=(32,32,3))\n    if model_num < 5:\n        x = basic_conv_block1(inp, 64, 3)\n        x = AveragePooling2D(2)(x)\n        x = basic_conv_block1(x, 128, 3)\n        x = AveragePooling2D(2)(x)\n        x = basic_conv_block1(x, 256, 3)\n        x = GlobalAvgPool2D()(x)\n        x = Dense(10, activation='softmax')(x)\n        model = Model(inp, x)\n    else:\n        x = basic_conv_block2(inp, 64, 3)\n        x = AveragePooling2D(2)(x)\n        x = basic_conv_block2(x, 128, 3)\n        x = AveragePooling2D(2)(x)\n        x = basic_conv_block2(x, 256, 3)\n        x = GlobalAvgPool2D()(x)\n        x = Dense(10, activation='softmax')(x)\n        model = Model(inp, x)\n    return model\n","metadata":{"execution":{"iopub.status.busy":"2021-07-07T03:29:40.607099Z","iopub.execute_input":"2021-07-07T03:29:40.607435Z","iopub.status.idle":"2021-07-07T03:29:40.619968Z","shell.execute_reply.started":"2021-07-07T03:29:40.607398Z","shell.execute_reply":"2021-07-07T03:29:40.619196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\ndef ensemble_average(models, X):\n    \"\"\"集合平均\n    Parameters: \n        models(list): 模型列表\n        X(array): 驗證資料\n    Returns : 個圖像的預測值\n    \"\"\"\n    preds_sum = None # 儲存驗證結果\n    for model in models: # 訓練好的模型\n        if preds_sum is None:\n            # 第一個模型的預測機率\n            # preds_sum列數為資料筆數，行數為類別數\n            preds_sum = model.predict(X)\n        else:\n            # 第二個模型開始累加預測機率\n            preds_sum += model.predict(X)\n    # 計算每筆資料屬於各類別的平均機率\n    probs = preds_sum / len(models)\n    # 機率最大值為輸出類別  \n    return np.argmax(probs, axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-07-07T03:29:40.621903Z","iopub.execute_input":"2021-07-07T03:29:40.622408Z","iopub.status.idle":"2021-07-07T03:29:40.63268Z","shell.execute_reply.started":"2021-07-07T03:29:40.62237Z","shell.execute_reply":"2021-07-07T03:29:40.631942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import Callback\n\nclass Checkpoint(Callback):\n    \"\"\"Callback 的子類別\n    Attributes:\n        model(object): 訓練中的模型\n        filepath(str): 儲存權重的資料夾路徑\n        best_val_acc : 目前最高準確率\n    \"\"\"\n\n    def __init__(self, model, filepath):\n        \"\"\"\n        Parameters:\n            model(Model): 訓練中的模型\n            filepath(str): 儲存權重的資料夾路徑\n            best_val_acc(int): 目前最高準確率\n        \"\"\"\n        self.model = model\n        self.filepath = filepath\n        self.best_val_acc = 0.0\n\n    def on_epoch_end(self, epoch, logs):\n        \"\"\"\n        重新定義訓練週期結束時所呼出的函式\n        從剛剛的訓練週期當中儲存準確率較高的權重\n        Parameters:\n            epoch(int): 訓練次數\n            logs(dict): {'val_acc': 損失 , 'val_acc': 準確率 }\n        \"\"\"\n        if self.best_val_acc < logs['val_acc']:\n            # 儲存比前一次的訓練準確率還要高的權重\n            self.model.save_weights(self.filepath)\n            # 儲存準確率\n            self.best_val_acc = logs['val_acc']\n            print('Weights saved.', self.best_val_acc)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-07T03:29:40.634741Z","iopub.execute_input":"2021-07-07T03:29:40.635324Z","iopub.status.idle":"2021-07-07T03:29:40.646069Z","shell.execute_reply.started":"2021-07-07T03:29:40.63529Z","shell.execute_reply":"2021-07-07T03:29:40.645226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\nimport pickle\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import LearningRateScheduler\nfrom tensorflow.keras.callbacks import History\n\ndef train(X_train, X_test, y_train, y_test, y_test_label):\n    \"\"\" \n    執行訓練\n    Parameters:\n    X_train(ndarray): 訓練資料\n    X_test(ndarray) : 測試資料\n    y_train(ndarray): 訓練資料的正確答案\n    y_test(ndarray) : 測試資料的正確答案\n    y_test_label(ndarray): 測試資料的正確答案\n    \"\"\"\n    n_estimators = 9 # 集成的模型數量\n    batch_size = 1024 # 批次大小\n    epoch = 80 # 訓練次數\n    models = [] # 模型的清單\n    # 各個模型的訓練歷程dict\n    global_hist = {\"hists\":[], \"ensemble_test\":[]}\n    # 初始化各個模型預測結果的2維矩陣\n    # (資料數量, 模型數量)\n    single_preds = np.zeros((X_test.shape[0],  # 列數為圖像張數\n                             n_estimators))    # 行數為模型數量\n\n    # 模型有幾個、就重複幾次\n    for i in range(n_estimators):\n        # 顯示現在是第幾個模型\n        print('Model',i+1) \n        # 建立卷積神經網路，引數為模型的編號 \n        train_model = create_cnn(i) \n        # 編譯模型 \n        train_model.compile(optimizer='adam', \n                            loss='categorical_crossentropy', \n                            metrics=[\"acc\"]) \n        # 將編譯後的模型追加到清單 \n        models.append(train_model) \n\n        # 建立回呼當中的History物件\n        hist = History()\n        # 建立回呼當中的Checkpoint物件\n        cp = Checkpoint(train_model,       # 模型\n                        f'weights_{i}.h5') # 儲存權重的檔名\n        # 步驟衰減函數\n        def step_decay(epoch):\n            initial_lrate = 0.001 # 基礎學習率\n            drop = 0.5            # 衰減率\n            epochs_drop = 10.0    # 每10次訓練週期執行步驟衰減\n            lrate = initial_lrate * math.pow(drop,\n                                             math.floor((1+epoch)/epochs_drop))\n            return lrate\n        \n        lrate = LearningRateScheduler(step_decay)\n\n        # 資料擴增\n        datagen = ImageDataGenerator(rotation_range=15,      # 在15度範圍內隨機旋轉\n                                     width_shift_range=0.1,  # 以圖像寬度的0.1比例隨機橫向移動\n                                     height_shift_range=0.1, # 以圖像高度的0.1比例隨機上下移動\n                                     horizontal_flip=True,   # 朝水平方向隨機翻轉、左右對調\n                                     zoom_range=0.2)         # 以原始尺寸的0.2倍比例隨機放大\n        \n        # 進行訓練\n        train_model.fit(datagen.flow(X_train,y_train, batch_size=batch_size), \n                        epochs=epoch, \n                        steps_per_epoch=X_train.shape[0] // batch_size, \n                        validation_data=(X_test, y_test), \n                        verbose=1, \n                        callbacks=[hist, cp, lrate]) # 回呼\n\n        # 讀入訓練完成的模型所獲得最高準確率時的權重\n        train_model.load_weights(f'weights_{i}.h5')\n\n        # 凍結模型的所有權重\n        for layer in train_model.layers:\n            layer.trainable = False\n\n        # 使用測試資料進行預測、求各個圖像當中標籤的最大值\n        # 求出每列的最大值\n        single_preds[:, i] = np.argmax(train_model.predict(X_test), axis=-1)\n\n        # 將訓練完成模型的訓練歷程登錄到global_hist\n        global_hist['hists'].append(hist.history)\n        \n        # 執行平均集成\n        ensemble_test_pred = ensemble_average(models, X_test)\n\n        # 使用scikit-learn.accuracy_score()取得集成的準確率\n        ensemble_test_acc = accuracy_score(y_test_label, ensemble_test_pred)\n\n        # 將集成準確率追加到global_hist\n        global_hist['ensemble_test'].append(ensemble_test_acc)\n        # 輸出現在的集成準確率\n        print('Current Ensemble Test Accuracy : ', ensemble_test_acc)\n\n    global_hist['corrcoef'] = np.corrcoef(single_preds, rowvar=False) # 求出每行的相關係數\n    print('Correlation predicted value')\n    print(global_hist['corrcoef'])\n","metadata":{"execution":{"iopub.status.busy":"2021-07-07T03:29:40.647353Z","iopub.execute_input":"2021-07-07T03:29:40.647878Z","iopub.status.idle":"2021-07-07T03:29:41.16849Z","shell.execute_reply.started":"2021-07-07T03:29:40.64784Z","shell.execute_reply":"2021-07-07T03:29:41.167701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 準備資料 \nX_train, X_test, y_train, y_test, y_test_label = prepare_data() \n# 執行集成\ntrain(X_train, X_test, y_train, y_test, y_test_label)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-07T03:29:41.169766Z","iopub.execute_input":"2021-07-07T03:29:41.170325Z","iopub.status.idle":"2021-07-07T08:32:35.276692Z","shell.execute_reply.started":"2021-07-07T03:29:41.170282Z","shell.execute_reply":"2021-07-07T08:32:35.275626Z"},"trusted":true},"execution_count":null,"outputs":[]}]}