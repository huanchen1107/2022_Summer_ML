{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nfrom tensorflow.keras.datasets import cifar10\nfrom tensorflow.keras.utils import to_categorical\n\ndef prepare_data():\n    \"\"\" \n    準備資料\n    Returns:\n        X_train(ndarray): 訓練資料 (50000.32.32.3)\n        X_test(ndarray): 測試資料 (10000.32.32.3)\n        y_train(ndarray): 將訓練資料改為One-hot encoding後的標籤 (50000,10)\n        y_train(ndarray): 將測試資料改為One-hot encoding後的標籤 (10000,10)\n        y_test_label(ndarray): 測試資料的正確答案 (10000)\n    \"\"\"\n    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n    # 對訓練資料與測試資料的圖像執行常規化\n    x_train, x_test = x_train.astype('float32'), x_test.astype('float32')\n    x_train, x_test = x_train/255.0, x_test/255.0\n    # 將訓練資料與測試資料的標籤轉換為以One-hot encoding來呈現10的類別\n    y_train, y_test = to_categorical(y_train), to_categorical(y_test)\n\n    return x_train, x_test, y_train, y_test\n","metadata":{"execution":{"iopub.status.busy":"2021-06-09T09:02:54.087958Z","iopub.execute_input":"2021-06-09T09:02:54.088409Z","iopub.status.idle":"2021-06-09T09:02:59.970987Z","shell.execute_reply.started":"2021-06-09T09:02:54.088305Z","shell.execute_reply":"2021-06-09T09:02:59.969854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D\nfrom tensorflow.keras import optimizers\n\ndef make_convlayer():\n    \"\"\" \n    建立模型\n    \"\"\"\n    # Sequential物件\n    model = Sequential()\n    # 卷積層 1\n    model.add(Conv2D(filters=64, \n                     kernel_size=3, \n                     padding='same',\n                     activation='relu', \n                     input_shape=(32,32,3)))\n    # 2 × 2 池化層\n    model.add(MaxPooling2D(pool_size=2))\n    # 卷積層 2\n    model.add(Conv2D(filters=128, \n                     kernel_size=3, \n                     padding='same',\n                     activation='relu'))\n    # 2 × 2 池化層\n    model.add(MaxPooling2D(pool_size=2))\n    # 卷積層 3\n    model.add(Conv2D(filters=256, \n                     kernel_size=3, \n                     padding='same',\n                     activation='relu')) \n    #2 × 2 池化層 \n    model.add(MaxPooling2D(pool_size=2)) \n    # 扁平層 \n    model.add(Flatten()) \n    # 丟棄法\n    model.add(Dropout(0.4))\n    # 第7層\n    model.add(Dense(512, activation='relu'))\n    # 輸出層\n    model.add(Dense(10, activation='softmax'))\n\n    # 優化器為 Adam\n    model.compile(loss=\"categorical_crossentropy\",\n                  optimizer=optimizers.Adam(lr=0.001),\n                  metrics=[\"accuracy\"])\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-06-09T09:02:59.972515Z","iopub.execute_input":"2021-06-09T09:02:59.972963Z","iopub.status.idle":"2021-06-09T09:02:59.983432Z","shell.execute_reply.started":"2021-06-09T09:02:59.972917Z","shell.execute_reply":"2021-06-09T09:02:59.98213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom tensorflow.keras.callbacks import Callback\nfrom tensorflow.keras import backend\n\nclass CyclicLR(Callback):\n    \"\"\"\n    Attributes:\n        lr_min(float) : 學習率下限\n        lr_max(float) : 學習率上限\n        step_size(int) : 步長\n        mode(str) : 學習率規劃選擇\n        gamma(float) : 指數衰減率\n        scale_fn(function) : 以lambda定義Scaling函數\n        clr_iterations(float) : 迭代次數 (視為循環性學習率的實施次數)\n        trn_iterations(float) : 迭代次數 (作為批次的反覆次數)\n        scale_mode(int) : 學習率規劃候選\n            # 0: 三角學習率 \n            # 1: 三角學習率，學習率減半 \n            # 2: 三角學習率，學習率指數衰減 \n    \"\"\" \n    \n    def __init__(self, lr_min, lr_max, step_size, mode, gamma=0.99994): \n        \"\"\" \n        Parameters: \n            lr_min(float) : 學習率下限 \n            lr_max(float) : 學習率上限\n            step_size(int) : 步長\n            mode(str) : 學習率規劃選擇\n            gamma(float) : 指數衰減率\n        \"\"\"\n        self.lr_min = lr_min       # 學習率下限\n        self.lr_max = lr_max       # 學習率上限\n        self.step_size = step_size # 步長 \n        self.mode = mode           # 學習率規劃選擇\n        self.gamma = gamma         # 指數衰減率\n        self.clr_iterations = 0.   # 循環性學習率的執行次數 \n        self.trn_iterations = 0.   # 迭代次數 \n        self.history = {}          # 記錄學習率與批次編號\n        self._init_scale(gamma)\n\n    def _init_scale(self, gamma):\n        \"\"\"\n        學習率的衰減方式\n        Parameters:\n            gamma(int): 指數衰減率\n        \"\"\"\n        # 三角學習率 \n        if self.mode == 0:\n            self.scale_fn = lambda x: 1.\n            self.scale_mode = 'cycle'\n        # 三角學習率，學習率減半 \n        elif self.mode == 1: \n            self.scale_fn = lambda x: 1 / (2. ** (x - 1)) \n            self.scale_mode = 'cycle' \n        # 三角學習率，學習率指數衰減\n        elif self.mode == 2:\n            self.scale_fn = lambda x: gamma ** (x)\n            self.scale_mode = 'iterations'\n    \n    def clr(self):\n        \"\"\"\n        計算循環性學習率\n        設定優化器的初始學習率\n        \"\"\" \n        cycle = np.floor(1 + self.clr_iterations / (2 * self.step_size)) \n        x = np.abs(self.clr_iterations / self.step_size - 2 * cycle + 1)\n        \n        if self.scale_mode == 'cycle': # 三角學習率、三角學習率，學習率減半\n            decay = np.maximum(0, (1 - x)) * self.scale_fn(cycle)\n            return self.lr_min + (self.lr_max - self.lr_min) * decay\n        else:                          # 三角學習率，學習率指數衰減\n            decay = np.maximum(0, (1 - x)) * self.scale_fn(self.clr_iterations)\n            return self.lr_min + (self.lr_max - self.lr_min) * decay\n\n    def on_train_begin(self, logs={}): \n        \"\"\"\n        設定訓練剛開始時所呼出的 優化器初始學習率\n        \"\"\"\n        logs = logs or {}\n        self.losses = []    # 記錄損失的清單 \n        self.lr = []        # 記錄學習率的清單 \n        # 設定優化器的初始學習率\n        if self.clr_iterations == 0:\n            backend.set_value(self.model.optimizer.lr, self.lr_min)\n        else:\n            backend.set_value(self.model.optimizer.lr, self.clr())\n\n    def on_batch_end(self, epoch, logs=None): \n        \"\"\"\n        當批次結束時就呼出\n        設定優化器學習率\n        將處理中的批次編號與學習率記錄在history\n        \"\"\" \n        logs = logs or {} \n        self.trn_iterations += 1 \n        self.clr_iterations += 1 \n        # 記錄現在的學習率 \n        self.history.setdefault('lr', []).append(backend.get_value(self.model.optimizer.lr)) \n        # 記錄現在的迭代數\n        self.history.setdefault('iterations', []).append(self.trn_iterations)\n        # logs:{'batch': 批次編號 } 的 dict 當中取出金鑰跟數值\n        for k, v in logs.items():\n            # history: 在{'batch': 批次編號清單} 當中追加現在的批次編號\n            self.history.setdefault(k, []).append(v)\n        # 執行clr()，設定優化器的學習率\n        backend.set_value(self.model.optimizer.lr, self.clr())\n","metadata":{"execution":{"iopub.status.busy":"2021-06-09T09:06:33.960565Z","iopub.execute_input":"2021-06-09T09:06:33.960955Z","iopub.status.idle":"2021-06-09T09:06:33.98084Z","shell.execute_reply.started":"2021-06-09T09:06:33.960924Z","shell.execute_reply":"2021-06-09T09:06:33.979656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import LearningRateScheduler, Callback\n\ndef train(x_train, x_test, y_train, y_test, mode=0): \n    \"\"\" Parameters: \n        x_train, x_test, y_train, y_test: 訓練以及驗證資料 \n        mode(int): 循環性學習率的模式 (0, 1, 2)\n    \"\"\" \n    batch_size = 128             # 小批次尺寸 \n    iteration = 50000            # 資料數\n    stepsize = iteration/128 * 4 # 循環步長\n    lr_min = 0.0001              # 學習率下限\n    lr_max = 0.001               # 學習率上限\n\n    # 建立CyclicLR 物件\n    clr_triangular = CyclicLR(mode=mode,          # 學習率規劃選擇\n                              lr_min=lr_min,      # 學習率下限\n                              lr_max=lr_max,      # 學習率上限\n                              step_size=stepsize) # 步長\n    # 將CyclicLR物件儲存到清單\n    callbacks_list = [clr_triangular]\n    # 建立模型\n    model = make_convlayer()\n\n    # 資料擴增\n    datagen = ImageDataGenerator(width_shift_range=0.1,    # 以圖像寬度的0.1比例隨機橫向移動\n                                 height_shift_range=0.1,   # 以圖像高度的0.1比例隨機上下移動\n                                 rotation_range=10,        # 在10度的範圍內隨機旋轉\n                                 zoom_range=0.1,           # 以0.1的比例隨機放大\n                                 horizontal_flip=True)     # 左右翻轉\n    # 訓練次數\n    epochs = 100\n    # 進行訓練\n    history = model.fit(\n        datagen.flow(x_train, y_train, batch_size=batch_size),\n        # 步驟次數是由小批次尺寸除以圖像張數所得出的整數值\n        steps_per_epoch=x_train.shape[0] // batch_size,\n        epochs=epochs,                    # 學習次數\n        verbose=1,                        # 輸出學習進度的狀況\n        validation_data=(x_test, y_test), # 驗證資料\n        callbacks=callbacks_list)\n\n    return history, clr_triangular\n","metadata":{"execution":{"iopub.status.busy":"2021-06-09T09:08:01.686218Z","iopub.execute_input":"2021-06-09T09:08:01.686575Z","iopub.status.idle":"2021-06-09T09:08:01.6957Z","shell.execute_reply.started":"2021-06-09T09:08:01.686546Z","shell.execute_reply":"2021-06-09T09:08:01.694752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train, x_test, y_train, y_test = prepare_data()","metadata":{"execution":{"iopub.status.busy":"2021-06-09T09:06:50.147262Z","iopub.execute_input":"2021-06-09T09:06:50.147671Z","iopub.status.idle":"2021-06-09T09:07:05.723867Z","shell.execute_reply.started":"2021-06-09T09:06:50.147633Z","shell.execute_reply":"2021-06-09T09:07:05.722894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nhistory_0, clr_triangular_0 = train(x_train, x_test, y_train, y_test, mode=0)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T09:08:10.492208Z","iopub.execute_input":"2021-06-09T09:08:10.492796Z","iopub.status.idle":"2021-06-09T09:56:18.706911Z","shell.execute_reply.started":"2021-06-09T09:08:10.492747Z","shell.execute_reply":"2021-06-09T09:56:18.705542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nhistory_1, clr_triangular_1 = train(x_train, x_test, y_train, y_test, mode=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T09:03:00.01526Z","iopub.status.idle":"2021-06-09T09:03:00.015705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nhistory_2, clr_triangular_2 = train(x_train, x_test, y_train, y_test, mode=2)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T09:03:00.016662Z","iopub.status.idle":"2021-06-09T09:03:00.017104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\nplt.figure(figsize=(10, 5)) # 繪圖尺寸\n# 繪製訓練資料的精確度\nplt.plot(history_0.history['accuracy'], label='CLR', linestyle = '--')\nplt.plot(history_1.history['accuracy'], label='CLR, Half Decay', linestyle = '-.')\nplt.plot(history_2.history['accuracy'], label='CLR, Exp Decay')\nplt.legend()            # 顯示圖例\nplt.grid()              # 顯示格線\nplt.xlabel('Epoch')     # x 軸標籤\nplt.ylabel('Train_Acc') # y 軸標籤\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-06-09T09:03:00.017912Z","iopub.status.idle":"2021-06-09T09:03:00.018305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 將驗證資料的精確度繪製圖成圖形\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.figure(figsize=(10, 5)) # 繪圖尺寸\n# 繪製訓練資料的精確度\nplt.plot(history_0.history['val_accuracy'], label='CLR', linestyle = '--')\nplt.plot(history_1.history['val_accuracy'], label='CLR, Half Decay', linestyle = '-.')\nplt.plot(history_2.history['val_accuracy'], label='CLR, Exp Decay')\nplt.legend()            # 顯示圖例\nplt.grid()              # 顯示格線\nplt.xlabel('Epoch')     # x 軸標籤\nplt.ylabel('Val_Acc')   # y 軸標籤\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-06-09T09:03:00.01916Z","iopub.status.idle":"2021-06-09T09:03:00.019549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10, 15))# 繪圖尺寸\n\nplt.subplot(3, 1, 1)        # 繪製於 3 × 1格線範圍的1\n# 繪製學習率\nplt.plot(clr_triangular_0.history['lr'], label='Learning Rate(CLR)')\nplt.legend()                # 顯示圖例\nplt.grid()                  # 顯示格線\nplt.xlabel('Iterations')    # x 軸標籤\nplt.ylabel('Learning Rate') # y 軸標籤\n\nplt.subplot(3, 1, 2)        # 繪製於3 × 1格線範圍的2\n# 繪製學習率\nplt.plot(clr_triangular_1.history['lr'], label='Learning Rate(CLR, Half Decay)')\nplt.legend()                # 顯示圖例\nplt.grid()                  # 顯示格線\nplt.xlabel('Iterations')    # x 軸標籤\nplt.ylabel('Learning Rate') # y 軸標籤\n\nplt.subplot(3, 1, 3)        # 繪製於3 × 1格線範圍的3\n# 繪製學習率\nplt.plot(clr_triangular_2.history['lr'], label='Learning Rate(CLR, Exp Decay)')\nplt.legend()                # 顯示圖例\nplt.grid()                  # 顯示格線\nplt.xlabel('Iterations')    # x 軸標籤\nplt.ylabel('Learning Rate') # y 軸標籤\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-06-09T09:03:00.020528Z","iopub.status.idle":"2021-06-09T09:03:00.02098Z"},"trusted":true},"execution_count":null,"outputs":[]}]}